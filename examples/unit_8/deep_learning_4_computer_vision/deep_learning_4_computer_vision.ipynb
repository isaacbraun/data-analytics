{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Convnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convnet takes as input tensors of shape (image_height, image_width, image_channels). In the following example, we’ll configure the convnet to process inputs of size (28, 28, 1), which is the format of MNIST images. We’ll do this by passing the argument input_shape=(28, 28, 1) to the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 11:15:48.739926: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-03 11:15:49.377449: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/administrator/miniconda3/envs/tf/lib/\n",
      "2023-04-03 11:15:49.377521: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/administrator/miniconda3/envs/tf/lib/\n",
      "2023-04-03 11:15:49.377527: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-04-03 11:15:51.399555: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-03 11:15:52.181901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14230 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:c1:00.0, compute capability: 8.6\n",
      "2023-04-03 11:15:52.182507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14230 MB memory:  -> device: 1, name: NVIDIA RTX A4000, pci bus id: 0000:e1:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from keras import layers \n",
    "from keras import models\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) \n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu')) \n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In deep learning, a convolutional neural network (CNN) is a class of artificial neural network most commonly applied to analyze visual imagery.\n",
    "- CNNs use a mathematical operation called convolution in place of general matrix multiplication in at least one of their layers.\n",
    "- They are specifically designed to process pixel data and are used in image recognition and processing. \n",
    "- They have applications in image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, natural language processing, brain–computer interfaces, and financial time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The convolution operation:\n",
    "\n",
    "Convolution layers learn local patterns: in the case of images, patterns found in small 2D windows of the inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![convolution](./convolution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first convolution layer will learn small local patterns such as edges, a second convolution layer will learn larger patterns made of the features of the first layers, and so on. This allows convnets to efficiently learn increasingly complex and abstract visual concepts (because the visual world is fundamentally spatially hierarchical)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![convolution](./spatial_hierarchy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![convolution](./conv1-1-750x456.png)\n",
    "\n",
    "Output Pixel value = (w1*px1 + w2*px2 + w3*px3 + w4*px4 + w5*px5 + w6*px6) + bias\n",
    "\n",
    "![convolution](./conv2-1.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One important thing to note here is that Convolutions occur per channel. An input image would generally consist of three channels: red, green, and blue. \n",
    "- The above example shows convolution operation happening over 2-dimension input image, however actual image is represented in 3-dimensions! \n",
    "- Depth = \"channels\" that represent the dimension of the image in 3D. \n",
    "- The Kernel is 2D, so we apply the 2D kernel to all two dimensions of 3D input image. Then, all dimensions of post kernel operation are merged to get output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation \n",
    "\n",
    "- The activation function of a node defines the output of that node given an input or set of inputs. \n",
    "- A standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input. \n",
    "    - This is similar to the linear perceptron in neural networks. \n",
    "- However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities.\n",
    "\n",
    "### Vanishing Gradient Problem\n",
    "\n",
    "- Sigmoid functions are used frequently in neural networks to activate neurons. \n",
    "    - It is a logarithmic function with a characteristic S shape. \n",
    "    - The output value of the function is between 0 and 1. \n",
    "    - The sigmoid function is used for activating the output layers in binary classification problems. \n",
    "    - It is calculated as follows:\n",
    "\n",
    "![sigmoid](./jacob-vanishing-gradient-1.png)\n",
    "\n",
    "- First derivatives of sigmoid functions are bell curves with values ranging from 0 to 0.25: \n",
    "\n",
    "![firstderivative](./jacob-vanishing-gradient-2.jpg)\n",
    "\n",
    "- In back propagation, the new weight(wnew) of a node is calculated using the old weight(wold) and product of the learning rate(ƞ) and gradient of the loss function.\n",
    "\n",
    "\n",
    "![gradient](./jacob-vanishing-gradient-5.jpg)\n",
    "\n",
    "![new weight](./jacob-vanishing-gradient-6.jpg)\n",
    "\n",
    "- With the chain rule of partial derivatives, we can represent gradient of the loss function as a product of gradients of all the activation functions of the nodes with respect to their weights. \n",
    "    - Therefore, the updated weights of nodes in the network depend on the gradients of the activation functions of each node.\n",
    "\n",
    "- For the nodes with sigmoid activation functions, we know that the partial derivative of the sigmoid function reaches a maximum value of 0.25. \n",
    "    -  <b>When there are more layers in the network, the value of the product of derivative decreases until at some point the partial derivative of the loss function approaches a value close to zero, and the partial derivative vanishes. </b> \n",
    "        - We call this the vanishing gradient problem.\n",
    "\n",
    "- With shallow networks, sigmoid function can be used as the small value of gradient does not become an issue. \n",
    "    - When it comes to deep neural networks, the vanishing gradient could have a significant impact on performance. \n",
    "        - The weights of the network remain unchanged as the derivative vanishes. \n",
    "        - During back propagation, a neural network learns by updating its weights and biases to reduce the loss function. \n",
    "            - In a network with vanishing gradient, the weights cannot be updated, so the network cannot learn.     \n",
    "                - The performance of the network will decrease as a result.\n",
    "\n",
    "### Method to Overcome the problem\n",
    "\n",
    "- Instead of sigmoid, use an activation function such as ReLU.\n",
    "    - Rectified Linear Units (ReLU) are activation functions that generate a positive linear output when they are applied to positive input values. \n",
    "        - If the input is negative, the function will return zero.\n",
    "\n",
    "![ReLU](./jacob-vanishing-gradient-7.jpg)\n",
    "\n",
    "- The derivative of a ReLU function is defined as 1 for inputs that are greater than zero and 0 for inputs that are negative. \n",
    "    - The graph shared below indicates the derivative of a ReLU function.\n",
    "\n",
    "![derivative of a ReLU function](./jacob-vanishing-gradient-8.png)\n",
    "\n",
    "- The partial derivative of the loss function will be having values of 0 or 1 which prevents the gradient from vanishing! :-)\n",
    "    - The use of ReLU function thus prevents the gradient from vanishing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Max-Pooling Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s display the architecture of the convnet so far. You can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the example, the size of the feature maps is halved after every MaxPooling2D layer. \n",
    "    - For instance, before the first MaxPooling2D layers, the feature map is 26 × 26, but the max-pooling operation halves it to 13 × 13. \n",
    "    - <b> That’s the role of max pooling: to aggressively downsample feature maps. </b>\n",
    "\n",
    "- In short, the reason to use downsampling is to reduce the number of feature-map coefficients to process, as well as to induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![max pooling](./max_pooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![average pooling](./average_pooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, this is similar to what we have:\n",
    "\n",
    "![convolution](./Basic-CNN-architecture-and-kernel-A-typical-CNN-consists-of-several-component-types.ppm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding a classifier on top of the convnet: \n",
    "    - First we have to flatten the 3D oputputs to 1D, and then add a few Dense layers on top.\n",
    "    - We'll do 10-way classification, using a final layer with 10 outputs and a softmax activation. \n",
    "        - In multiclass classification the softmax activation is often used.\n",
    "\n",
    "        ![softmax](./Softmax-function-image.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                36928     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "#As you can see, the (3, 3, 64) outputs are flattened into vectors of shape (576,) \n",
    "#before going through two Dense layers.\n",
    "#A dense layer is a layer that is deeply connected with its preceding layer which means the neurons \n",
    "#of the layer are connected to every neuron of its preceding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train the convnet on MINIST images\n",
    "\n",
    "- For each pixel you would need 3 scalars (each for one channel), so it would be 60000 (# images) x 28x28 x 3 (# channels).\n",
    "\n",
    "- And how many channels you need when the image is in greyscale? Just one, so it would be 60000 x 28 x 28 x 1.\n",
    "\n",
    "- Also, we’ll preprocess the data by reshaping it into the shape the network expects and scaling it so that all values are in the [0, 1] interval. \n",
    "    - Previously, our training images, were stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval. \n",
    "        - We transform it into a float32 array of shape (60000, 28 * 28) with values between 0 and 1.\n",
    "\n",
    "- sparse_categorical_crossentropy: Used as a loss function for multi-class classification model where the output label is assigned integer value.\n",
    "\n",
    "![firstderivative](./1*Y2KPVGrVX9MQkeI8Yjy59Q.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 4s 3ms/step - loss: 0.0059 - accuracy: 0.9982\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0057 - accuracy: 0.9983\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0043 - accuracy: 0.9985\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0033 - accuracy: 0.9989\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0037 - accuracy: 0.9989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd487f2c0a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0327 - accuracy: 0.9910\n",
      "Test accuracy: 0.991\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Border Effects and Padding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Padding is the addition of (typically) 0-valued pixels on the borders of an image. \n",
    "    - This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. \n",
    "\n",
    "- In the figure below, we pad a 3 × 3 input, increasing its size to 5 × 5. \n",
    "    - The corresponding output then increases to a 4 × 4 matrix. \n",
    "    - The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: 0 × 0 + 0 × 1 + 0 × 2 + 0 × 3 = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![padding](./padding.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
